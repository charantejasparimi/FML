{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization using Pythons split() function**"
      ],
      "metadata": {
        "id": "70nONfcAa2pf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATdlzgQZ--7s",
        "outputId": "2741fb57-2e49-4f66-df93-0c37d80132b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking?', 'up1', '12', 'a_&', 'string', 'into', 'tokens.', '(Commonly,', 'these', 'tokens', 'are', 'words,', 'numbers,', 'and/or', 'punctuation.', 'The', 'tensorflow_textpackage', 'provides', 'a', 'number', 'of', 'tokenizers', 'available', 'for', 'preprocessing', 'text', 'required', 'by', 'your', 'text-based', 'models.', 'By', 'performing', 'the', 'tokenization', 'in', 'the', 'TensorFlow', 'graph,', 'you', 'will', 'not', 'need', 'to', 'worry'] \n",
            " 50 \n",
            "\n",
            "['Tokenization is the process of breaking? up1 12 a_& string into tokens', ' (Commonly, these tokens are words, numbers, and/or punctuation', ' The tensorflow_textpackage provides a number of tokenizers available for preprocessing text required by your text-based models', ' By performing the tokenization in the TensorFlow graph, you will not need to worry'] \n",
            " 4\n"
          ]
        }
      ],
      "source": [
        "text =\"Tokenization is the process of breaking? up1 12 a_& string into tokens. (Commonly, these tokens are words, numbers, and/or punctuation. The tensorflow_textpackage provides a number of tokenizers available for preprocessing text required by your text-based models. By performing the tokenization in the TensorFlow graph, you will not need to worry\"\n",
        "# word tokenization\n",
        "token_1 =text.split()\n",
        "print(token_1,\"\\n\",len(token_1),\"\\n\")\n",
        "sentence_1=text.split(\".\")\n",
        "# sentence tokenization\n",
        "print(text.split(\".\"),\"\\n\",len(sentence_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization using Regular Expressions (RegEx)**"
      ],
      "metadata": {
        "id": "L1ka0igla8HQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re \n",
        "# regular expression    find all occurance  w : words,n.o,_   + one or  more times occured\n",
        "token_2=re.findall(\"[\\w']+\", text)  \n",
        "print(token_2)\n",
        "print(\"\\n\",len(token_2))\n",
        "\n",
        "\n",
        "# The re.compile('[.?!] ') part creates a regular expression pattern that matches any period, exclamation mark, or question mark followed by a space character\n",
        "sentences_2= re.compile('[.?!]').split(text)\n",
        "print(sentences_2,\"\\n\",len(sentences_2))\n",
        "print(\"No.of sentences : \", len(sentences_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtXkcUxCCfF4",
        "outputId": "1705b59f-4dac-43ad-bce3-f9e5b398bc70"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'up1', '12', 'a_', 'string', 'into', 'tokens', 'Commonly', 'these', 'tokens', 'are', 'words', 'numbers', 'and', 'or', 'punctuation', 'The', 'tensorflow_textpackage', 'provides', 'a', 'number', 'of', 'tokenizers', 'available', 'for', 'preprocessing', 'text', 'required', 'by', 'your', 'text', 'based', 'models', 'By', 'performing', 'the', 'tokenization', 'in', 'the', 'TensorFlow', 'graph', 'you', 'will', 'not', 'need', 'to', 'worry']\n",
            "\n",
            " 52\n",
            "['Tokenization is the process of breaking', ' up1 12 a_& string into tokens', ' (Commonly, these tokens are words, numbers, and/or punctuation', ' The tensorflow_textpackage provides a number of tokenizers available for preprocessing text required by your text-based models', ' By performing the tokenization in the TensorFlow graph, you will not need to worry'] \n",
            " 5\n",
            "No.of sentences :  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization using NLTK**"
      ],
      "metadata": {
        "id": "BiESo6oObOqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "token_3 = word_tokenize(text)\n",
        "print(token_3,\"\\n\",len(token_3),\"\\n\")\n",
        "\n",
        "from nltk.tokenize import sent_tokenize\n",
        "sentence_3 = sent_tokenize(text)\n",
        "print(sentence_3,\"\\n\",len(sentence_3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5Em4NMjF0Qm",
        "outputId": "1d137d35-9426-41f6-84a1-3d706757af8e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', '?', 'up1', '12', 'a_', '&', 'string', 'into', 'tokens', '.', '(', 'Commonly', ',', 'these', 'tokens', 'are', 'words', ',', 'numbers', ',', 'and/or', 'punctuation', '.', 'The', 'tensorflow_textpackage', 'provides', 'a', 'number', 'of', 'tokenizers', 'available', 'for', 'preprocessing', 'text', 'required', 'by', 'your', 'text-based', 'models', '.', 'By', 'performing', 'the', 'tokenization', 'in', 'the', 'TensorFlow', 'graph', ',', 'you', 'will', 'not', 'need', 'to', 'worry'] \n",
            " 60 \n",
            "\n",
            "['Tokenization is the process of breaking?', 'up1 12 a_& string into tokens.', '(Commonly, these tokens are words, numbers, and/or punctuation.', 'The tensorflow_textpackage provides a number of tokenizers available for preprocessing text required by your text-based models.', 'By performing the tokenization in the TensorFlow graph, you will not need to worry'] \n",
            " 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}